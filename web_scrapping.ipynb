{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iRcjG5pIUVze",
        "outputId": "c8293c16-e1ec-4171-9f07-73d48116557c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "2ynKAPpNUQ3r"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import requests\n",
        "import json\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "main = 'https://mydramalist.com'\n",
        "for j in range(251,500):\n",
        "  dictionary = {}\n",
        "  rank = []\n",
        "  name = []\n",
        "  link = []\n",
        "  new_link = []\n",
        "\n",
        "  url = 'https://mydramalist.com/shows/top?page=251'\n",
        "\n",
        "  page = requests.get(url)\n",
        "  soup = BeautifulSoup(page.text, 'html')\n",
        "  titles = soup.find_all('h6', class_=\"text-primary title\")\n",
        "  ranking  = soup.find_all('div', class_=\"ranking pull-right\")\n",
        "  rank = [r.text[1:] for r in ranking]\n",
        "  a_title = [s.find_all('a')[0] for s in titles]\n",
        "  link = [h['href'] for h in a_title]\n",
        "  name = [n.text.strip() for n in a_title]\n",
        "  new_link = [main + l for l in link]\n",
        "  for i in range(len(name)):\n",
        "      second = requests.get(new_link[i])\n",
        "      soup1 = BeautifulSoup(second.text, 'html')\n",
        "      catogory = soup1.find_all('li', class_=\"list-item p-a-0 content-rating\")\n",
        "      cat = soup1.find_all('li', class_=\"list-item p-a-0\")\n",
        "      dictionary[str(rank[i])] = {'name' : str(name[i]),\n",
        "                                  'country' : str(cat[4].text),\n",
        "                                  'type' : str(cat[5].text),\n",
        "                                  'episodes' : str(cat[6].text),\n",
        "                                  'rating' : str(cat[11].text),\n",
        "                                  'rank' : str(cat[12].text),\n",
        "                                  'popularity' : str(cat[13].text),\n",
        "                                  'content' : str(catogory[0].text)\n",
        "                                  }\n",
        "  file_path = f'/content/drive/MyDrive/web_scrapping/my_list_{j+1}.json'\n",
        "  with open(file_path, 'w') as file:\n",
        "      json.dump(dictionary, file)\n",
        "\n",
        "  print(f\"finished: {j+1} page\")"
      ],
      "metadata": {
        "id": "r5r7DRFyUVC1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "directory = '/content/drive/MyDrive/web_scrapping'\n",
        "\n",
        "combined_data = {}\n",
        "\n",
        "\n",
        "for filename in os.listdir(directory):\n",
        "    if filename.endswith(\".json\"):\n",
        "        file_path = os.path.join(directory, filename)\n",
        "\n",
        "        with open(file_path, 'r') as file:\n",
        "            data = json.load(file)\n",
        "\n",
        "        combined_data.update(data)\n",
        "\n",
        "output_file_path = '/content/drive/MyDrive/web_scrapping/output/combined_data.json'\n",
        "with open(output_file_path, 'w') as output_file:\n",
        "    json.dump(combined_data, output_file, indent=2)\n",
        "\n",
        "print(f\"Combined data has been saved to {output_file_path}\")"
      ],
      "metadata": {
        "id": "IMqtumnHY9wZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('/content/drive/MyDrive/web_scrapping/output/combined_data.json', 'r') as file:\n",
        "    data = json.load(file)"
      ],
      "metadata": {
        "id": "idkUhorhZamf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# filtering data get the relevent data\n",
        "filtered_data = {key: value for key, value in data.items() if value['content'] == 'Content Rating: 15+ - Teens 15 or older'}"
      ],
      "metadata": {
        "id": "sv2YaEHhZI9J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for key, value in filtered_data.items():\n",
        "    print(value['name'])"
      ],
      "metadata": {
        "id": "iBf_BjjeZeab"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}